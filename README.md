# Vision Transformer (ViT) Implementation: An Image is Worth 16x16 Words

> "We show that... a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks." â€” *Dosovitskiy et al.*

## ðŸš€ Project Overview
This repository contains a PyTorch implementation of the groundbreaking **Vision Transformer (ViT)**, replicated from scratch.

![Vision Transformer Architecture](https://media.geeksforgeeks.org/wp-content/uploads/20250108160202257232/Vision-Transformer-Architecture_.webp)


In a landscape dominated by Convolutional Neural Networks (CNNs), ViT demonstrated that the inductive biases of convolutions (translation invariance and locality) are not strictly necessary for state-of-the-art performance. This project explores how treating images as **sequences of flattened patches** allows standard Transformer encoders to solve vision tasks with global receptive fields from the very first layer.

## ðŸ›  Key Implementations
* **Patch Embeddings:** Implemented a custom projection layer to decompose inputs into 16x16 flattened patches, transforming 2D image data into 1D sequences.
* **Learnable Positional Encodings:** Engineered learnable embeddings to retain spatial information within the permutation-invariant Transformer architecture.
* **Multi-Head Self-Attention (MSA):** Built the core attention mechanism allowing the model to weigh the importance of different image regions dynamically.
* **End-to-End Pipeline:** Full training loop, including data augmentation, learning rate scheduling, and metric tracking.

## ðŸ“Š Results & Attention Maps
*Unlike CNNs, which aggregate features locally, the ViT mixes information globally.*

Below are the attention maps generated by the model, visualizing exactly which parts of the image the model focuses on during classification:

